---
always_allow_html: yes
output:
  md_document:
    variant: markdown_github
---

##### Setup.

```{r setup}
library("dplyr")
library("plotly")
library("WVPlots")  # needs dev version from Github
library("microbenchmark")

set.seed(224)

mkData <- function(n) {
  d <- data.frame(x= runif(n),   # predictor
                  y= rnorm(n)>=0 # outcome/truth
  )
  d$cx <- 1-d$x # complement of x
  d$ny <- !d$y  # not y
  d
}
d <- mkData(5)
print(d)
```


## The effect

We are looking at the facts that 
`AUC(prediction, outcome) + AUC(1-prediction, outcome) == 1`
and 
`AUC(prediction, outcome) + AUC(prediction, !outcome) == 1`.

```{r plotly}
plotlyROC <- function(predictions, target, title) {
  rocFrame <- WVPlots::graphROC(predictions, target)
  plot_ly(rocFrame$pointGraph, x = ~FalsePositiveRate, y = ~TruePositiveRate, 
        type='scatter', mode='lines+markers', hoverinfo= 'text', 
        text= ~ paste('threshold:', model, 
                      '</br>FalsePositiveRate:', FalsePositiveRate,
                      '</br>TruePositiveRate:', TruePositiveRate)) %>%
    layout(title= title)
}

plotlyROC(d$x, d$y, 'interactive version of base plot (when rendered to html)')
```


```{r WVPlots}
WVPlots::ROCPlot(d,'x','y',TRUE,'base plot')
WVPlots::ROCPlot(d,'cx','y',TRUE,'complemented x')
WVPlots::ROCPlot(d,'x','ny',TRUE,'negated y')
```


##### `ModelMetrics`


```{r ModelMetrics}
ModelMetrics::auc(d$y, d$x) +
  ModelMetrics::auc(d$y, d$cx)

ModelMetrics::auc(d$y, d$x) +
  ModelMetrics::auc(d$ny, d$x)
```


##### `sigr`

```{r sigr}

sigr::calcAUC(d$x, d$y) + 
  sigr::calcAUC(d$cx, d$y)
sigr::calcAUC(d$x, d$y) +
  sigr::calcAUC(d$x, d$ny)
```

##### `ROCR`

```{r ROCR}
aucROCR <- function(predictions, target) {
  pred <- ROCR::prediction(predictions,
                           target)
  perf_AUC <- ROCR::performance(pred,
                                "auc")
  perf_AUC@y.values[[1]]
}

aucROCR(d$x, d$y) + 
  aucROCR(d$cx, d$y)
aucROCR(d$x, d$y) + 
  aucROCR(d$x, d$ny)
```

##### `AUC`

```{r AUC}
aucAUC <- function(predictions, target) {
  AUC::auc(AUC::roc(predictions, 
                  factor(ifelse(target, "1", "0"))))
}

aucAUC(d$x, d$y) +
  aucAUC(d$cx, d$y)
aucAUC(d$x, d$y) +
  aucAUC(d$x, d$ny)
```

##### `caret`

```{r caret}
aucCaret <- function(predictions, target) {
  classes <- c("class1", "class2")
  df <- data.frame(obs= ifelse(target, 
                               "class1", 
                               "class2"),
                 pred = ifelse(predictions>0.5, 
                               "class1", 
                               "class2"),
                 class1 = predictions,
                 class2 = 1-predictions)
  caret::twoClassSummary(df, lev=classes)[['ROC']]
}


aucCaret(d$x, d$y) + 
  aucCaret(d$cx, d$y)
aucCaret(d$x, d$y) + 
  aucCaret(d$x, d$ny)
```


##### `pROC`

```{r pROC}
pROC::auc(y~x, d) + 
  pROC::auc(y~cx, d) # wrong
```

More tries with `pROC`.

From `help(roc)` 

> direction	
>
> in which direction to make the comparison? "auto" (default): automatically define in which group the median is higher and take the direction accordingly. ">": if the predictor values for the control group are higher than the values of the case group (controls > t >= cases). "<": if the predictor values for the control group are lower or equal than the values of the case group (controls < t <= cases).

```{r pROCretry}
pROC::auc(y~x, d, direction= '<') + 
  pROC::auc(y~cx, d, direction= '<')
pROC::auc(y~x, d, direction= '<') +
  pROC::auc(ny~x, d, direction= '<')
```


### Timing

```{r timing}


dTime <- mkData(100)
ModelMetrics::auc(dTime$y, dTime$x)
sigr::calcAUC(dTime$x, dTime$y)
aucROCR(dTime$x, dTime$y)
aucAUC(dTime$x, dTime$y)
aucCaret(dTime$x, dTime$y)
pROC::auc(y~x, dTime, direction= '<')

sizes <- 2^(7:16)
rlist <- lapply(sizes,
                function(si) {
                  dTime <- mkData(si)
                  res <- microbenchmark(
                    ModelMetrics::auc(dTime$y, dTime$x),
                    sigr::calcAUC(dTime$x, dTime$y),
                    aucROCR(dTime$x, dTime$y),
                    aucAUC(dTime$x, dTime$y),
                    aucCaret(dTime$x, dTime$y),
                    pROC::auc(y~x, dTime, direction= '<'),
                    times=10L
                  )
                  res$size <- si
                  res
                })
res <- bind_rows(rlist)
# select down columns and control units
res %>% 
  group_by(expr,size) %>%
  mutate(time = time/1e9) %>% # move from NanoSeconds to seconds
  summarize(meanTimeS = mean(time), 
            q1 = quantile(time, probs=0.25),
            q2 = quantile(time, probs=0.5),
            q3 = quantile(time, probs=0.75)) %>%
  arrange(size,expr) -> plt
plt$expr <- reorder(plt$expr, -plt$q2)

# from: http://stackoverflow.com/a/22227846/6901725
base_breaks <- function(n = 10){
    function(x) {
        axisTicks(log10(range(x, na.rm = TRUE)), log = TRUE, n = n)
    }
}

ggplot(plt, aes(x=size, color=expr, fill=expr)) +
  geom_point(aes(y=meanTimeS)) +
  geom_line(aes(y=q2)) +
  geom_ribbon(aes(ymin=q1, ymax=q3), alpha=0.3, color=NA) +
  scale_x_log10(breaks= base_breaks()) + 
  scale_y_log10(breaks= base_breaks()) +
  ylab("time in seconds") +
  ggtitle("run times by AUC package and data size")
```



