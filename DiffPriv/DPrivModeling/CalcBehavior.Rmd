---
title: "CalcBehavior"
output: html_document
---

An attempt to semi-theoretically analyze the efficacy of different nested training
methods.

In this "d" is our training data frame.  We will use it to model y~group (group being a string-valued variable) for every possible y vector (y is TRUE/FALSE so there are 2^nrow(d) such frames).  We want to estimate the out of sample performance of our model, so we evaluate on a new data frame called dTest (which has exactly one row per level).

```{r}
set.seed(23225)
k <- 2   # number of groups, we are going to use group id to predict y
gGroups <- paste('g',1:k,sep='')
d <- data.frame(group=rep(gGroups,2),
                y=NA,
                stringsAsFactors=FALSE)
print(d)

n <- nrow(d)
print(n)

dTest <- data.frame(group=gGroups,
                    stringsAsFactors=FALSE)
print(dTest)
```


We are going to form estimates of the form P(y=TRUE|group).  The model will
be est_group is some sort of empirical estimate per-group derived from training
and then the overall model is of the form P(y|group)~s(b*est_group) where s is the sigmoid
function and b is estimated by logistic regression.  For this case (a single variable)
this second outer model is not needed.  We also could alternately model a logistic regression directly on the group labels.

What we are trying to do is make a simple example showing the problems one can run
into when building a nested model which is a method used when the group variable takes
on a very large number of values (such as zip codes) (see http://www.win-vector.com/blog/2012/07/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/ ).  We are deliberately looking for the over-fit that comes from such a nested model, and what methods decrease this undesirable effect (see: http://winvector.github.io/DiffPriv/DiffPriv.pdf ).

The idea in this exercise is we are going to have the unknown true probablity P(y|group) as identically x (x, being a variable we vary from 0 to 1) for all groups.  The inner conditoned estimate (called "est") is estimated one of a few ways including:

 * Naive or empirical esimate: est_group = N(y|group)/N(group)
 * Jackknifed estimate: est_group = (N(y|group)-selfY)/(N(group)-1)
 * Half Jackknifed: est_group = (N(y|group)-selfY/2)/(N(group)-1/2)
 * Split: est_group = N(y|group in first half of training data)/N(group in first half of training data)

The second stage (or overall model) is then simulated using logistic regression (R's glm() command).  We estimate P(y|group) as s(b*group_est) (b being the fit coefficient, for the split case we use only the disjoint second half of the data for estimation of b).

The idea is: in many cases the naive or empirical estimate is already over-fitting (or memorizing training data).  So we would would like the outter modeling procedure to correct this (or at least not make this worse).  However, in the naive case the outter fitting procedure is pretty much forced to pick a coefficient b=1 and does not correct.  However, if there is some de-coupling of the empirical estimates the fitter is allowed to see from the actual training data (such as Jackknife, split procedures, or even differential privacy established through the introduction of noise as in slide 18 of http://www.slideshare.net/SessionsEvents/misha-bilenko-principal-researcher-microsoft ) then we get better generalization performance (b is picked less than 1).

It is the relative strength or statistical effiency of each of these techniques we wish to demonstrate.  To do this we are going to assume the group labels of our data set are fixed (they being the fixed experimental design) and then assume we see the y-lables with y=TRUE with probablity x (x being a parameter we will vary from 0 through 1).  Because for each x there are exactly 2^nrow(d) possible y assignments and we can exactly compute the probability of any y assignment we can sum over every possible observed data set weighted according to the exact probability of each data set occuring.   For each possible observed data set we can compute the outcomes and we thus know the exact probabilities of each possible outcome.  In fact we can write these probabilities and expectations as polynomials in x, allowing us to leave x as a parameter to be explored later (versus having to substitue in a specific value).  In this manner we are going to estimate the following summaries of the fits (as functions of x):

 * The expected value of max_group s(b*group_est) over all models. That is: for each composite model (empirical estimate plus outter model) we find which group it things has the largest probabilty and report this probability.
 * The expected value of min_group s(b*group_est) the smallest prediction
 * The expected value of mean_group s(b*group_est) the expected mid-prediction
 
In all cases the expectation is taken over all possible realizaitons of the traning vector y with y=TRUE having probability x (x a variable to be filled in later).


The first step is estimating y conditioned on group, the second is a 
glm() model on top of this estimate.  The nested model usually gives trouble as the
group variable can hide degrees of freedom and cause over-fitting.




```{r}
library('polynom')
library('ggplot2')



#' compute the direct expected value of y per group
#'
#' @param d data frame with y and group columns
#' @return named vector with grouped y-means
empiricalEst <- function(d) {
  d$one <- 1.0
  num <- aggregate(y~group,data=d,FUN=sum)
  den <- aggregate(one~group,data=d,FUN=sum)
  if(!all(all.equal(num$group,den$group)==TRUE)) {
    stop("name mismatch")
  }
  est <- num$y/den$one
  names(est) <- num$group
  est
}

#' compute the jackknifed expected value of y per group
#'
#' @param d data frame with y and group columns
#' @param jackDenom denominator of current observation to pull off in estimate, 1 is standard jackknife, 2 is one half jackknife
#' @return named vector with jackknifed grouped y-means
jackknifeEst <- function(d,jackDenom) {
  d$one <- 1.0
  num <- aggregate(y~group,data=d,FUN=sum)
  numM <- num$y
  names(numM) <- num$group
  numV <- numM[d$group]
  den <- aggregate(one~group,data=d,FUN=sum)
  denM <- den$one
  names(denM) <- den$group
  denV <- denM[d$group]
  if(!all(all.equal(num$group,den$group)==TRUE)) {
    stop("name mismatch")
  }
  v <- as.numeric((numV-d$y/jackDenom)/(denV-1/jackDenom))
  v[is.na(v)|is.infinite(v)|is.nan(v)] <- 0
  v
}

#' Fit a glm() on top of the dEstimate column to the d$y column, then apply this model to dTest
#'
#' Simulates the second stage of a 2-stage modeling process.
#' @param d data frame with y column
#' @param dEst numeric with one sub-model prediction per row of d
#' @param dTest frame to score on has group and est columns
#' @return dTest predictions
estimateExpectedPrediction <- function(d,dEst,dTest) {
  # catch cases unsafe for glm
  if(all(d$y) || all(!d$y) || 
     ((max(dEst)-min(dEst))<=1.0e-5)) {
    return(dTest$est)
  }
  d$est <- dEst
  m <- glm(y~0+est,data=d,family=binomial(link='logit'))
  predict(m,newdata=dTest,type='response')
}


naiveModel <- function(d,dTest) {
  as.numeric(empiricalEst(d)[dTest$group])
}

jackknifeModel <- function(d,dTest) {
  estimateExpectedPrediction(d,jackknifeEst(d,1),dTest)
}

jackknifeModel2 <- function(d,dTest) {
  estimateExpectedPrediction(d,jackknifeEst(d,2),dTest)
}

#' Split train nested model.  Assumes design can be split in half
#'
#' Simulates the second stage of a 2-stage modeling process.
#' @param d data frame with y column and group column
#' @param dTest frame to score on has group and est columns
#' @return dTest predictions
splitEstimateExpectedPrediction <- function(d,dTest) {
  isA <- logical(nrow(d))
  isA[seq_len(floor(nrow(d)/2))] <- TRUE
  dA <- d[isA,]
  dB <- d[!isA,]
  if(!all(all.equal(sort(unique(dA$group)),
                    sort(unique(dB$group)))==TRUE)) {
    stop("bad split")
  }
  dB$est <- empiricalEst(dA)[dB$group]
  # catch cases unsafe for glm
  if(all(dB$y) || all(!dB$y) || 
     ((max(dB$est)-min(dB$est))<=1.0e-5)) {
    return(dTest$est)
  }
  m <- glm(y~0+est,data=dB,family=binomial(link='logit'))
  predict(m,newdata=dTest,type='response')
}




#' evaluate a strategy by returning summary statistics
#'
#' @param d training data frame
#' @param dTest evaluation data frame
#' @param strat strategy to apply
#' @param what name of strategy
#' @return scores
evalModelingStrategy <- function(d,dTest,strat,what) {
  predExpectedMin <- 0
  predExpectedMean <- 0
  predExpectedMax <- 0
  probOn <- polynomial(c(0,1))  # polnomial x representing chance of y=TRUE
  # run through each possible realization of the training outcome
  # vector y.  Each situation is weigthed by the probability of 
  # seeing this y-vector (though the outcomes in the situation
  # are conditionally indpendent of this unknown probabilty 
  # given the realized y).
  ys <- expand.grid( rep( list(0:1), n))==1
  for(ii in seq_len(nrow(ys))) {
    y <- as.logical(ys[ii,])
    d$y <- y
    py <- probOn^(sum(y))*(1-probOn)^(length(y)-sum(y))
    dTest$est <- as.numeric(empiricalEst(d)[dTest$group])
    pred <- strat(d,dTest)
    predExpectedMin <- predExpectedMin + min(pred)*py
    predExpectedMean <- predExpectedMean + mean(pred)*py
    predExpectedMax <- predExpectedMax + max(pred)*py
  }
  x <- seq(0,1,by=0.01)
  plotD <- data.frame(x=x,
                      what=what,
                      expectedMin=as.function(predExpectedMin)(x),
                      expectedMean=as.function(predExpectedMean)(x),
                      expectedMax=as.function(predExpectedMax)(x),
                      stringsAsFactors = FALSE)
  plotD$expectedRatioMin <- plotD$expectedMin/plotD$x
  plotD$expectedRatioMean <- plotD$expectedMean/plotD$x
  plotD$expectedRatioMax <- plotD$expectedMax/plotD$x
  list(
    plotD=plotD,
    predExpectedMin=predExpectedMin,
    predExpectedMean=predExpectedMean,
    predExpectedMax=predExpectedMax
  )
}




plotD2 <- evalModelingStrategy(d,dTest,naiveModel,
                               'naive prediction')$plotD
plotD3 <-  evalModelingStrategy(d,dTest,jackknifeModel,
                                'jackknife prediction')$plotD
plotD4 <-  evalModelingStrategy(d,dTest,jackknifeModel2,
                                'jackknife 2 prediction')$plotD
plotD5 <- evalModelingStrategy(d,dTest,splitEstimateExpectedPrediction,
                               'split prediction')$plotD

ggplot(data=rbind(plotD2,plotD3),mapping=aes(x=x,
                                             y=expectedMean,
                                             ymin=expectedMin,ymax=expectedMax,
                              color=what,fill=what)) +
  geom_ribbon(alpha=0.5) +
  geom_abline() + 
  geom_line() +
  coord_fixed() +
  ylab('expectated estimates') +
  ggtitle(paste('predictions, n=',n))

ggplot(data=rbind(plotD3,plotD4),
       mapping=aes(x=x,
                   y=expectedMean,
                   ymin=expectedMin,ymax=expectedMax,
                              color=what,fill=what)) +
  geom_ribbon(alpha=0.5) +
  geom_abline() + 
  geom_line() +
  ylab('expectated estimates') +
  coord_fixed() +
  ggtitle(paste('predictions, n=',n))

ggplot(data=rbind(plotD3,plotD5),
       mapping=aes(x=x,
                   y=expectedMean,
                   ymin=expectedMin,ymax=expectedMax,
                              color=what,fill=what)) +
  geom_ribbon(alpha=0.5) +
  geom_abline() + 
  geom_line() +
  ylab('expectated estimates') +
  coord_fixed() +
  ggtitle(paste('predictions, n=',n))

plotDA <- rbind(plotD2,plotD3,plotD4,plotD5)
plotDA$expectedMaxError <- pmax(abs(plotDA$x-plotDA$expectedMin),
                               abs(plotDA$expectedMax-plotDA$x))
ggplot(data=plotDA,mapping=aes(x=x,y=expectedMaxError,
                              color=what,fill=what,linetype=what)) +
  geom_line() +
  ylab('expectated error') +
  ggtitle(paste('max error in estimates, n=',n))



ggplot(data=rbind(plotD2,plotD3),
       mapping=aes(x=x,
                   y=expectedRatioMean,
                   ymin=expectedRatioMin,ymax=expectedRatioMax,
                   color=what,fill=what)) +
  geom_ribbon(alpha=0.5) +
  geom_line() + 
  geom_hline(yintercept=1) +
  ylab('expected ratios') + 
  ggtitle(paste('prediction ratios, n=',n))

ggplot(data=rbind(plotD3,plotD4),
       mapping=aes(x=x,
                   y=expectedRatioMean,
                   ymin=expectedRatioMin,ymax=expectedRatioMax,
                   color=what,fill=what)) +
  geom_ribbon(alpha=0.5) +
  geom_line() +
  geom_hline(yintercept=1) +
  ylab('expected ratios') + 
  ggtitle(paste('prediction ratios, n=',n))

ggplot(data=rbind(plotD3,plotD5),
       mapping=aes(x=x,
                   y=expectedRatioMean,
                   ymin=expectedRatioMin,ymax=expectedRatioMax,
                   color=what,fill=what)) +
  geom_ribbon(alpha=0.5) +
  geom_line() +
  geom_hline(yintercept=1) +
  ylab('expected ratios') + 
  ggtitle(paste('prediction ratios, n=',n))

plotDA <- rbind(plotD2,plotD3,plotD4,plotD5)
plotDA$expectedMaxAlbsLogRelError <- pmax(abs(log(plotDA$expectedRatioMin)),
                               abs(log(plotDA$expectedRatioMax)))
ggplot(data=plotDA,
       mapping=aes(x=x,
                   y=expectedMaxAlbsLogRelError,
                   color=what,fill=what,linetype=what)) +
  geom_line() +
  ylab('expected abs log error ratios') + 
  ggtitle(paste('prediction expected abs log error ratio, n=',n))

```

