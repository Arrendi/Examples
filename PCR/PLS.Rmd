---
title: "Partial Least Squares"
author: "John Mount, Win-Vector LLC"
date: "June 20, 2016"
output:
  md_document:
    variant: markdown_github
---

Partial least squares on similar data to [Principal Components Regression, Pt. 2: Y-Aware Methods](https://github.com/WinVector/Examples/blob/master/PCR/YAwarePCA.md).


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=7,
                      echo=TRUE, warning=FALSE, message=FALSE)

library('ggplot2')
library('tidyr')
library('WVPlots') # devtools::install_github('WinVector/WVPlots',build_vignettes=TRUE)

barbell_plot = function(frame, xvar, ymin, ymax, colorvar=NULL) {
  if(is.null(colorvar)) {
    gplot = ggplot(frame, aes_string(x=xvar))
  } else {
    gplot = ggplot(frame, aes_string(x=xvar, color=colorvar))
  }
  
  gplot + geom_point(aes_string(y=ymin)) + 
    geom_point(aes_string(y=ymax)) +
    geom_linerange(aes_string(ymin=ymin, ymax=ymax)) +
    ylab("value")
}

dotplot_identity = function(frame, xvar, yvar, colorvar=NULL) {
  if(is.null(colorvar)) {
    gplot = ggplot(frame, aes_string(x=xvar, y=yvar, ymax=yvar))
  } else {
    gplot = ggplot(frame, 
                   aes_string(x=xvar, y=yvar, ymax=yvar, 
                              color=colorvar))
  }
  gplot + geom_point() + geom_linerange(aes(ymin=0))
}

extractProjection <- function(ndim,princ) {
  # pull off the rotation.  
  proj <- princ$rotation[,1:ndim] 
  # sign was arbitrary, so flip in convenient form
  for(i in seq_len(ndim)) {
    si <- sign(mean(proj[,i]))
    if(si!=0) {
      proj[,i] <- proj[,i]*si
    }
  }
  proj
}

rsq <- function(x,y) {
  1 - sum((y-x)^2)/sum((y-mean(y))^2)
}
```


```{r mkdata}
# build example where even and odd variables are bringing in noisy images
# of two different signals.
mkData <- function(n) {
  for(group in 1:50) {
    # y is the sum of two effects yA and yB
    yA <- rnorm(n)
    yB <- rnorm(n)
    if(group==1) {
      d <- data.frame(y=yA+yB+rnorm(n))
      code <- 'x'
    } else {
      code <- paste0('noise',group-1)
    }
    yS <- list(yA,yB)
    # these variables are correlated with y in group 1,
    # but only to each other (and not y) in other groups
    for(i in 1:5) {
      vi <- yS[[1+(i%%2)]] + rnorm(nrow(d))
      d[[paste(code,formatC(i,width=2,flag=0),sep='.')]] <- ncol(d)*vi
    }
  }
  d
}
```


```{r makedata}
# make data
set.seed(23525)
dTrain <- mkData(1000)
dTest <- mkData(1000)
```

Use *y*-aware scaling.

```{r yaware}
ncores <- parallel::detectCores()
pClus <- parallel::makeCluster(ncores)
vars <- setdiff(colnames(dTrain),'y')
print(length(vars))
formula <- paste('y',paste(vars,collapse=' + '),sep=' ~ ')
pruneSig = NULL # leaving null to prevent (useful) pruning, in practice set to 1/length(vars) or some such.
useCrossMethod <- TRUE
if(useCrossMethod) {
  cfe <- vtreat::mkCrossFrameNExperiment(dTrain,vars,'y',scale=TRUE,parallelCluster=pClus)
  treatmentPlan <- cfe$treatments
  newvars <- treatmentPlan$scoreFrame$varName
  dmTrain <- as.matrix(cfe$crossFrame[,newvars])
} else {
  treatmentPlan <- vtreat::designTreatmentsN(dTrain,vars,'y',verbose=FALSE,parallelCluster=pClus)
  newvars <- treatmentPlan$scoreFrame$varName
  dmTrain <-  as.matrix(vtreat::prepare(treatmentPlan,dTrain,scale=TRUE,pruneSig=pruneSig)[,newvars],
                        parallelCluster=pClus)
}
print(length(newvars))
dmTest <- as.matrix(vtreat::prepare(treatmentPlan,dTest,scale=TRUE,pruneSig=pruneSig)[,newvars],
                     parallelCluster=pClus)
princ <- prcomp(dmTrain, center = FALSE, scale. = FALSE)
proj <- extractProjection(2,princ)
projectedTrain <- as.data.frame(dmTrain %*% proj,
                      stringsAsFactors = FALSE)
projectedTrain$y <- dTrain$y
projectedTest <- as.data.frame(dmTest %*% proj,
                      stringsAsFactors = FALSE)
projectedTest$y <- dTest$y
model <- lm(y~PC1+PC2,data=projectedTrain)
projectedTrain$pred <- predict(model,newdata = projectedTrain)
projectedTest$pred <- predict(model,newdata = projectedTest)

ScatterHist(projectedTrain,'pred','y',paste('y-aware 2 component model on train'),
            smoothmethod='identity',annot_size=3)
trainrsq <- rsq(projectedTrain$pred,projectedTrain$y)
print(paste("train rsq",trainrsq))

ScatterHist(projectedTest,'pred','y',paste('y-aware 2 component model on test'),
            smoothmethod='identity',annot_size=3)
testrsq <- rsq(projectedTest$pred,projectedTest$y)
print(paste("test rsq",testrsq))
parallel::stopCluster(pClus)
```

Use latent components to model (partial least squares).

```{r plsN}
library("pls")
vars <- setdiff(colnames(dTrain),'y')
plotTrain <- dTrain
plotTest <- dTest
formula <- paste('y',paste(vars,collapse=' + '),sep=' ~ ')
for(ncomp in c(2,5,10)) {
  print("###################")
  print(paste('ncomp',ncomp))
  modelN <- plsr(as.formula(formula), ncomp = ncomp, data = dTrain, 
                 scale=TRUE, validation = "CV")
  plotTrain$plsNpred <- as.numeric(predict(modelN,newdata=dTrain,ncomp=ncomp,type='response'))
  ScatterHist(plotTrain,'plsNpred','y',paste('pls',ncomp,'model on train'),
              smoothmethod='identity',annot_size=3)
  trainrsq <- rsq(plotTrain$plsNpred,plotTrain$y)
  print(paste("ncomp",ncomp,"train rsq",trainrsq))
  plotTest$plsNpred <-as.numeric(predict(modelN,newdata=dTest,ncomp=ncomp,type='response'))
  ScatterHist(plotTest,'plsNpred','y',paste('pls',ncomp,'model on test'),
              smoothmethod='identity',annot_size=3)
  testrsq <- rsq(plotTest$plsNpred,plotTest$y)
  print(paste("ncomp",ncomp,"test rsq",testrsq))
  print("###################")
}
```


