---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

# Why `replyr`

`replyr` stands for **RE**mote **PLY**ing of big data for **R**.

Why should [R](https://www.r-project.org) users try [`replyr`](https://CRAN.R-project.org/package=replyr)?  Because it lets you take a number of common working patterns and apply them to remote data (such as databases or [`Spark`](https://spark.apache.org)). 

`replyr` allows users to work with `Spark` data similar to how they work with local `data.frame`s. Some key capability gaps remedied by `replyr` include:

  * Summarizing data: `replyr_summary()`.
  * Binding tables by row: `replyr_bind_rows()`.
  * Using the split/apply/combine pattern (`dplyr::do()`): `replyr_split()`, `replyr::gapply()`.
  * Pivot/anti-pivot (`gather`/`spread`): `replyr_moveValuesToRows()`/ `replyr_moveValuesToColumns()`.
  * Parametric programming (`wrapr::let()` and `replyr::replyr_apply_f_mapped()`).
  * Handle tracking.

You may have already learned to decompose your local data processing into steps including the above,
so retaining such capabilities makes working with `Spark` and [`sparklyr`](http://spark.rstudio.com) *much* easier.

Below are some examples.

***

# Examples

***

```{r setup}
base::date()
# devtools::install_github('rstudio/sparklyr')
# devtools::install_github('tidyverse/dplyr')
# devtools::install_github('tidyverse/dbplyr')
# devtools::install_github('WinVector/replyr')
suppressPackageStartupMessages(library("dplyr"))
packageVersion("dplyr")
packageVersion("dbplyr")
library("tidyr")
packageVersion("tidyr")
library("replyr")
packageVersion("replyr")
suppressPackageStartupMessages("spaklyr")
packageVersion("sparklyr")

sc <- sparklyr::spark_connect(version='2.0.2', 
                              master = "local")
```

## `summary`

Standard `summary()`,  `glimpse()`, `glance()`, all fail on `Spark`.

```{r sparksummary, error=TRUE}
mtcars_spark <- copy_to(sc, mtcars)

# gives summary of handle, not data
summary(mtcars_spark)

# errors-out
glimpse(mtcars_spark)
```

```{r glance, error=TRUE}
packageVersion("broom")
broom::glance(mtcars_spark)
```

`replyr_summary` works.

```{r rsum}
replyr_summary(mtcars_spark) %>%
  select(-lexmin, -lexmax, -nunique, -index)
```

***

## `gather`/`spread`

`tidyr` pretty much only works on local data.

```{r gatherspread, error=TRUE}
mtcars2 <- mtcars %>%
  mutate(car = row.names(mtcars)) %>%
  copy_to(sc, ., 'mtcars2')

# errors out
mtcars2 %>% 
  tidyr::gather('fact', 'value')
```

(Note: we have been regularly seeing a segfault at the block below.  I think this is an issues I have filed against both [`replyr`](https://github.com/WinVector/replyr/issues/4) and [`sparklyr`](https://github.com/rstudio/sparklyr/issues/721).

```
*** caught segfault ***
address 0x0, cause 'unknown'

Traceback:
 1: r_replyr_bind_rows(lst, colnames, tempNameGenerator)
 2: replyr_bind_rows(rlist, tempNameGenerator = tempNameGenerator)
```

This pretty much can *not* be `replyr`'s fault as `replyr` is a pure `R` package that mereley issues commands to `dplyr`/`sparklyr`/`Spark`.  So at worst `replyr` is generate a complicated sequence that is exposing a bug in one of these or between once of all of these.  At this point I am pretty much decided on suggesting this markdown as a cluster acceptance test.  If you can't do this, you can't expect to later do complicated work.
)


```{r movevaluestorows, eval=TRUE}
mtcars2 %>%
  replyr_moveValuesToRows(nameForNewKeyColumn= 'fact', 
                          nameForNewValueColumn= 'value', 
                          columnsToTakeFrom= colnames(mtcars),
                          nameForNewClassColumn= 'class') %>%
  arrange(car, fact)
```



## `replyr_bind_rows`

`dplyr` `bind_rows`, `union`, and `union_all` are all currently unsuitable for use on `Spark`.
`replyr::replyr_union_all()` and `replyr::replyr_bind_rows()` supply working alternatives.

### `bind_rows()`


```{r bindrows, error=TRUE}
db1 <- copy_to(sc, 
               data.frame(x=1:2, y=c('a','b'), 
                          stringsAsFactors=FALSE),
               name='db1')
db2 <- copy_to(sc, 
               data.frame(y=c('c','d'), x=3:4, 
                          stringsAsFactors=FALSE),
               name='db2')

# Errors out as it tries to operate on the handles instead of the data.
bind_rows(list(db1, db2))
```

### `union_all`

```{r uniona, error=TRUE}
# ignores column names and converts all data to char
union_all(db1, db2)
```

### `union`

```{r union, error=TRUE}
# ignores column names and converts all data to char
# also will probably lose duplicate rows
union(db1, db2)
```

### `replyr_bind_rows`

`replyr::replyr_bind_rows` can bind multiple `data.frame`s together.

```{r replyr_bind_rows, error=TRUE}
replyr_bind_rows(list(db1, db2))
```

## `dplyr::do`

Our example is just taking a few rows from each group of a grouped data set. 
Note: since we are not enforcing order by an arrange we 
can't expect the results to always match on database
or `Spark` data sources.

### `dplyr::do` on local data

From `help('do', package='dplyr')`:

```{r dplyrdolocal}
by_cyl <- group_by(mtcars, cyl)
do(by_cyl, head(., 2))
```

***

### `dplyr::do` on `Spark`

```{r dplyrdolocalspark}
by_cyl <- group_by(mtcars_spark, cyl)
do(by_cyl, head(., 2))
```

Notice we did not get back usable results.

### `replyr` split/apply

```{r replyrdo}
mtcars_spark %>%
  replyr_split('cyl', 
               partitionMethod = 'extract') %>%
  lapply(function(di) head(di, 2)) %>%
  replyr_bind_rows()
```

### `replyr` `gapply`

```{r replyrgapply}
mtcars_spark %>%
  gapply('cyl',
         partitionMethod = 'extract',
         function(di) head(di, 2))
```

***

## `wrapr::let`

`wrapr::let` allows execution of arbitrary code with substituted variable names (note this is subtly different than binding values for names as with `base::substitute` or `base::with`).  This allows the user to write arbitrary `dplyr` code in the case of ["parametric variable names"](http://www.win-vector.com/blog/2016/12/parametric-variable-names-and-dplyr/) (that is when variable names are not known at coding time, but will become available later at run time as values in other variables) without directly using the `dplyr` "underbar forms"  (and the direct use of `lazyeval::interp` and `.dots=stats::setNames` to use the `dplyr` "underbar forms").

Example:

```{r  message=FALSE,results='hide',warning=FALSE}
library('dplyr')
```
```{r letexample}
# nice parametric function we write
ComputeRatioOfColumns <- function(d,NumeratorColumnName,DenominatorColumnName,ResultColumnName) {
  wrapr::let(
    alias=list(NumeratorColumn=NumeratorColumnName,
               DenominatorColumn=DenominatorColumnName,
               ResultColumn=ResultColumnName),
    expr={
      # (pretend) large block of code written with concrete column names.
      # due to the let wrapper in this function it will behave as if it was
      # using the specified paremetric column names.
      d %>% mutate(ResultColumn = NumeratorColumn/DenominatorColumn)
    })
}

# example data
d <- data.frame(a=1:5, b=3:7)

# example application
d %>% ComputeRatioOfColumns('a','b','c')
```

`wrapr::let` makes construction of abstract functions over `dplyr` controlled data much easier.  It is designed for the case where the "`expr`" block is large sequence of statements and pipelines.

`wrapr::let` is based on `gtools::strmacro` by Gregory R. Warnes.

***

## `replyr::replyr_apply_f_mapped`

`wrapr::let` was only the secondary proposal in the original [2016 "Parametric variable names" article](http://www.win-vector.com/blog/2016/12/parametric-variable-names-and-dplyr/).  What we really wanted was a stack of view so the data pretended to have names that matched the code (i.e., re-mapping the data, not the code).  

With a bit of thought we can achieve this if we associate the data re-mapping with a function environment instead of with the data.  So a re-mapping is active as long as a given controlling function is in control.  In our case that function is `replyr::replyr_apply_f_mapped()` and works as follows:

Suppose the operation we wish to use is a rank-reducing function that has been supplied as function from somewhere else that we do not have control of (such as a package).  The function could be simple such as the following, but we are going to assume we want to use it without alteration (including the without the small alteration of introducing `wrapr::let()`).

```{r rankfn}
# an external function with hard-coded column names
DecreaseRankColumnByOne <- function(d) {
  d$RankColumn <- d$RankColumn - 1
  d
}
```

To apply this function to `d` (which doesn't have the expected column names!) we use  `replyr::replyr_apply_f_mapped()` to create a new parameterized adapter as follows:

```{r replyrmapf}
# our data
d <- data.frame(Sepal_Length = c(5.8,5.7),
                Sepal_Width = c(4.0,4.4),
                Species = 'setosa',
                rank = c(1,2))

# a wrapper to introduce parameters
DecreaseRankColumnByOneNamed <- function(d, ColName) {
  replyr::replyr_apply_f_mapped(d, 
                                f = DecreaseRankColumnByOne, 
                                nmap = c(RankColumn = ColName),
                                restrictMapIn = FALSE, 
                                restrictMapOut = FALSE)
}

# use
dF <- DecreaseRankColumnByOneNamed(d, 'rank')
print(dF)
```

`replyr::replyr_apply_f_mapped()` renames the columns to the names expected by `DecreaseRankColumnByOne` (the mapping specified in `nmap`), applies `DecreaseRankColumnByOne`, and then inverts the mapping before returning the value.

***

## Handle management

Many [`Sparklyr`](https://CRAN.R-project.org/package=sparklyr) tasks involve creation of intermediate or temporary tables.  This can be through `dplyr::copy_to()` and through `dplyr::compute()`. These handles can represent a reference leak and eat up resources.   

Note: right now `sparklyr` `compute()` is buggy and [doesn't do anything useful](http://www.win-vector.com/blog/2017/06/managing-intermediate-results-when-using-rsparklyr/).

To help control handle lifetime the [`replyr`](https://CRAN.R-project.org/package=replyr) supplies record-retaining temporary name generators (and uses the same internally).

The actual function is pretty simple:

```{r printtr}
print(replyr::makeTempNameGenerator)
```

For instance to join a few tables it can be a good idea to call `compute` after each join for some data sources (else the generated `SQL` can become large and unmanageable).  This sort of code looks like the following (now hanging with `sparklyr `0.5.6.9003` June 22, 2017):

```{r joinlist, eval=TRUE}
# create example data
names <- paste('table', 1:5, sep='_')
tables <- lapply(names, 
                 function(ni) {
                   di <- data.frame(key= 1:3)
                   di[[paste('val',ni,sep='_')]] <- runif(nrow(di))
                   copy_to(sc, di, ni)
                 })

# build our temp name generator
tmpNamGen <- replyr::makeTempNameGenerator('JOINTMP')

# left join the tables in sequence
joined <- tables[[1]]
for(i in seq(2,length(tables))) {
  ti <- tables[[i]]
  if(i<length(tables)) {
    joined <- compute(left_join(joined, ti, by='key'),
                    name= tmpNamGen())
  } else {
    # use non-temp name.
    joined <- compute(left_join(joined, ti, by='key'),
                    name= 'joinres')
  }
}

# clean up temps
temps <- tmpNamGen(dumpList = TRUE)
print(temps)
for(ti in temps) {
  db_drop_table(sc, ti)
}

# show result
print(joined)
```

Careful introduction and management of materialized intermediates can conserve resources (both time and space) and greatly improve outcomes.  We feel it is a good practice to set up an explicit temp name manager, pass it through all your `Sparklyr` transforms, and then clear temps in batches after the results no longer depend on the intermediates.

***

# Conclusion

If you are serious about `R` controlled data processing in `Spark` you should seriously consider using `replyr` in addition to [`dplyr`](https://CRAN.R-project.org/package=dplyr) and `sparklyr`.

Be aware of the functionality we demonstrated depends on using the development version of `replyr`.  Though we will, of course, advance the CRAN version as soon as practical. 

Note: all of the above was demonstrated using the released CRAN 0.5.0 version of `dplyr` (not the [2017-05-30 `0.6.0` release candidate](https://github.com/tidyverse/dplyr/commit/c7ca37436c140173a3bf0e7f15d55b604b52c0b4), or the [2017-06-05 `0.7.0` release candidate](https://github.com/tidyverse/dplyr/commit/43dc94e88a4ab5938618b612bc9ec874de571598)) .  The assumption is that *some* of the work-arounds may become less necessary as we go forward (`glimpse()` and `glance()` in particular are likely to pick up `Spark` capabilities).  We kept with the 0.5.0 production `dplyr` as our experience is: the 0.6.0 version does not currently fully inter-operate with the [CRAN released version of `sparklyr` (0.5.5 2017-05-26)](https://CRAN.R-project.org/package=sparklyr) and other database sources (please see [here](https://github.com/tidyverse/dplyr/issues/2825), [here](https://github.com/tidyverse/dplyr/issues/2823), [here](https://github.com/rstudio/sparklyr/issues/678), and [here](https://github.com/tidyverse/dplyr/issues/2776) for some of the known potentially upgrade blocking issues).  While the [current development version of `sparklyr`](https://github.com/rstudio/sparklyr/commit/d981cd54326b5663b7311d5f30adeec68dacd1fe) does incorporate some improvements, it does not appear to be specially marked or tagged as release candidate.  

I'll probably re-run this worksheet after these packages get new CRAN releases.

***

```{r cleanup}
sparklyr::spark_disconnect(sc)
rm(list=ls())
gc()
```
