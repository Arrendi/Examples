---
title: "Spark Handles"
output: html_document
---

```{r setup}
library("sparklyr")
#packageVersion('sparklyr')
library("dplyr")
#packageVersion('dplyr')


# Please see the following video for installation help
#  https://youtu.be/qnINvPqcRvE
# spark_install(version = "2.1.0")

# set up a local "practice" Spark instance
sc <- spark_connect(master = "local",
                    version = "2.1.0")
#print(sc)
```

```{r builddata}
# build notional data, but do not
# leave it in the system (so we can
# demonstrate loading).
names <- vapply(1:3,
                function(i) {
                  di <- data.frame(x=runif(10))
                  ni <- paste('data', sprintf("%02d", i), sep='_')
                  hi <- copy_to(sc, di, 
                                name= ni,
                                overwrite= TRUE)
                  spark_write_parquet(hi, path= ni)
                  dplyr::db_drop_table(sc, ni)
                  ni
                },
                character(1))
```


```{r loaddata}
# load the parquet files to register them
# to the Spark cluster.
# Could keep handles at this point, 
#  or get them with:
#   DBI::dbListTables(sc)/dplyr::tbl(sc, namei)
tableCollection <- data_frame(name= names)
tableCollection$handle <- lapply(
  tableCollection$name,
  function(namei) {
    spark_read_parquet(sc, namei, namei)
  }
)
```

```{r finddata}
# or if we want to grab all tables
# sparklyr already knows about:
tableCollection <- data_frame(name = DBI::dbListTables(sc))
tableCollection$handle <-
  lapply(tableCollection$name,
         function(namei) {
          dplyr::tbl(sc, namei)
         })
```

```{r workwithdata}
# and names to handles for convenience
names(tableCollection$handle) <-
  tableCollection$name

# look at the top of each table (also forces
# evaluation!).
lapply(tableCollection$handle, 
       head)

# get dimensions of each table
lapply(tableCollection$handle, 
       dim)
```

```{r cleanup}
rm(list=ls())
gc()
```