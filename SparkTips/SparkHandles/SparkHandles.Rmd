
---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->


When using `R` to work over a big data system (such as `Spark`) much of your work is over "data handles" and not actual data (data handles are objects that control access to remote data).

Data handles are a lot like sockets or file-handles in that they can
not be safely serialized and restored (i.e., you can not save them
into a `.RDS` file and then restore them into another session).  This
means when you are starting or re-starting a project you must "ready"
all of your data references.  Your projects will be much easier to
manage and document if you load your references using the methods we
show below.

Let's set-up our example `Spark` cluster:

```{r setup}
library("sparklyr")
#packageVersion('sparklyr')
suppressPackageStartupMessages(library("dplyr"))
#packageVersion('dplyr')
suppressPackageStartupMessages(library("tidyr"))

# Please see the following video for installation help
#  https://youtu.be/qnINvPqcRvE
# spark_install(version = "2.0.2")

# set up a local "practice" Spark instance
sc <- spark_connect(master = "local",
                    version = "2.0.2")
#print(sc)
```

Data is much easier to manage than code, and much easier to compute
over.  So the more information you can keep as pure data the better
off you will be.  In this case we are loading the chosen names and
paths of `parquet` data we wish to work with from an external file
that is easy for the user to edit.


```{r loaddata}
# Read user's specification of files and paths.
userSpecification <- read.csv('tableCollection.csv',
                             header = TRUE,
			     strip.white = TRUE,
			     stringsAsFactors = FALSE)
print(userSpecification)
```

We can now read these `parquet` files (usually stored in `Hadoop`)
into our `Spark` environment as follows.

```{r readp}
readParquets <- function(userSpecification) {
  userSpecification <- as_data_frame(userSpecification)
  userSpecification$handle <- lapply(
    seq_len(nrow(userSpecification)),
    function(i) {
      spark_read_parquet(sc, 
                         name = userSpecification$tableName[[i]], 
                         path = userSpecification$tablePath[[i]])
    }
  )
  userSpecification
}

tableCollection <- readParquets(userSpecification)
print(tableCollection)
```

A `data.frame` is a great place to keep what you know about 
your `Spark` handles in one place.  Let's add some details
to our `Spark` handles.

```{r adddetails}
addDetails <- function(tableCollection) {
  tableCollection <- as_data_frame(tableCollection)
  # get the references
  tableCollection$handle <-
    lapply(tableCollection$tableName,
           function(tableNamei) {
             dplyr::tbl(sc, tableNamei)
           })
  
  # and tableNames to handles for convenience
  # and printing
  names(tableCollection$handle) <-
    tableCollection$tableName
  
  # add in some details (note: nrow can be expensive)
  tableCollection$nrow <- vapply(tableCollection$handle, 
                                 nrow, 
                                 numeric(1))
  tableCollection$ncol <- vapply(tableCollection$handle, 
                                 ncol, 
                                 numeric(1))
  tableCollection
}

tableCollection <- addDetails(userSpecification)

# convenient printing
print(tableCollection)

# look at the top of each table (also forces
# evaluation!).
lapply(tableCollection$handle, 
       head)
```

A particularly slick trick is to expand the columns
column into a taller table that allows us to quickly
identify which columns are in which tables.

```{r buildcolumnmap}
columnDictionary <- function(tableCollection) {
  tableCollection$columns <- 
    lapply(tableCollection$handle,
           colnames)
  columnMap <- tableCollection %>% 
    select(tableName, columns) %>%
    unnest(columns)
  columnMap
}

columnMap <- columnDictionary(tableCollection)
print(columnMap)
```

The idea is: place all of the above functions into a shared script or
package, and then use them to organize loading your `Spark` data
references.  With this practice you will have much less "spaghetti
code", better document intent, and have a versatile workflow.

The principles we are using are:

 * Keep configuration out of code (i.e., maintain the file list in a spreadsheet).  This makes working with others much easier.
 * Treat configuration as data (i.e., make sure the configuration is a nice regular table so that you can use `R` tools such as `tidyr::unnest()` to work with it).

```{r wrongwaytosave, eval=FALSE, include=FALSE}
# do not save handles, they are not really re-loadable
# in all situations.
saveRDS(tableCollection, 
        file='tableCollectionWrong.RDS')
```




```{r cleanup}
spark_disconnect(sc)
rm(list=ls())
gc()
```