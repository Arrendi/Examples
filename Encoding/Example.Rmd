---
title: "EncodingExample"
author: "Win-Vector LLC"
date: "4/14/2017"
output:
  md_document:
    variant: markdown_github
---

Model matrix does not store its "one-hot" plan anywhere, so you can not
safely assume the same formula applied to two different data sets (say train
and application or test) are using compatible encodings!

```{r modelmatrixexample}
dTrain <- data.frame(x= c('a','b','c'), 
                     stringsAsFactors = FALSE)
stats::model.matrix(~x, dTrain)
dTest <- data.frame(x= c('b','c'), 
                     stringsAsFactors = FALSE)
stats::model.matrix(~x, dTest)
```

Let's try the Titanic data set to see encoding in action.  `xgboost` requires a numeric matrix for its input, so unlike many `R` modeling methods we must manage the data encoding ourselves (instead of leaving that to `R` which often hides the encoding plan in the trained model).

```{r modelingexample}
library("titanic")
library("xgboost")
library("sigr")
library("WVPlots")

# select example data set
data(titanic_train)
str(titanic_train)
summary(titanic_train)
outcome <- 'Survived'
target <- 1
shouldBeCategorical <- c('PassengerId', 'Pclass', 'Parch')
for(v in shouldBeCategorical) {
  titanic_train[[v]] <- as.factor(titanic_train[[v]])
}
tooDetailed <- c("Ticket", "Cabin", "Name", "PassengerId")
vars <- setdiff(colnames(titanic_train), c(outcome, tooDetailed))

dTrain <- titanic_train



set.seed(3425656)
crossValPlan <- vtreat::kWayStratifiedY(nrow(dTrain), 
                                        10, 
                                        dTrain, 
                                        outcome)

evaluateModelingProcedure <- function(xMatrix, outcomeV, crossValPlan) {
  preds <- rep(NA_real_, nrow(xMatrix))
  for(ci in crossValPlan) {
    nrounds <- 1000
    cv <- xgb.cv(data= xMatrix[ci$train, ],
                 label= outcomeV[ci$train],
                 objective= 'binary:logistic',
                 nrounds= nrounds,
                 verbose= 0,
                 nfold= 5)
    #nrounds  <- which.min(cv$evaluation_log$test_rmse_mean) # regression
    nrounds  <- which.min(cv$evaluation_log$test_error_mean) # classification
    model <- xgboost(data= xMatrix[ci$train, ],
                     label= outcomeV[ci$train],
                     objective= 'binary:logistic',
                     nrounds= nrounds,
                     verbose= 0)
    preds[ci$app] <-  predict(model, xMatrix[ci$app, ])
  }
  preds
}
```

Our preferred way to encode data is to use the `vtreat` package either in the "no variables mode" shown below or in the "y aware" modes we usually teach.

```{r vtreatZ}
library("vtreat")
set.seed(3425656)
tplan <- vtreat::designTreatmentsZ(dTrain, vars, verbose=FALSE)
sf <- tplan$scoreFrame
newvars <- sf$varName[sf$code %in% c('clean', 'lev', 'isBad')]
trainVtreat <- as.matrix(vtreat::prepare(tplan, dTrain, 
                                         varRestriction = newvars))
print(dim(trainVtreat))
print(colnames(trainVtreat))
dTrain$predVtreatZ <- evaluateModelingProcedure(trainVtreat,
                                                       dTrain[[outcome]]==target,
                                                       crossValPlan)
sigr::permTestAUC(dTrain, 
                  'predVtreatZ',
                  outcome, target)
WVPlots::ROCPlot(dTrain, 
                 'predVtreatZ', 
                 outcome, target, 
                 'vtreat encoder performance')
```

Model matrix can perform similar encoding when we only have a single data set.

```{r modelmatrix}
set.seed(3425656)
f <- paste('~ 0 + ', paste(vars, collapse = ' + '))
# model matrix skips rows with NAs by default,
# get control of this through an option
oldOpt <- getOption('na.action')
options(na.action='na.pass')
trainModelMatrix <- stats::model.matrix(as.formula(f), 
                                  dTrain)
# note model.matrix does not conveniently store the encoding
# plan, so you may run into difficulty if you were to encode
# new data which didn't have all the levels seen in the training
# data.
options(na.action=oldOpt)
print(dim(trainModelMatrix))
print(colnames(trainModelMatrix))

dTrain$predModelMatrix <- evaluateModelingProcedure(trainModelMatrix,
                                                     dTrain[[outcome]]==target,
                                                     crossValPlan)
sigr::permTestAUC(dTrain, 
                  'predModelMatrix',
                  outcome, target)
WVPlots::ROCPlot(dTrain, 
                 'predModelMatrix', 
                 outcome, target, 
                 'model.matrix encoder performance')
```

`caret` supplies an encoding functionality properly split between training (`caret::dummyVars`) and application (called `predict()`).

```{r caret}
library("caret")
set.seed(3425656)
f <- paste('~', paste(vars, collapse = ' + '))
encoder <- caret::dummyVars(as.formula(f), dTrain)
trainCaret <- predict(encoder, dTrain)
print(dim(trainCaret))
print(colnames(trainCaret))

dTrain$predCaret <- evaluateModelingProcedure(trainCaret,
                                                     dTrain[[outcome]]==target,
                                                     crossValPlan)
sigr::permTestAUC(dTrain, 
                  'predCaret',
                  outcome, target)
WVPlots::ROCPlot(dTrain, 
                 'predCaret', 
                 outcome, target, 
                 'caret encoder performance')
```

You can also try y-aware encoding, but it isn't adding much in this situation.

```{r vtreatC}
set.seed(3425656)
# for y aware evaluation must cross-validate whole procedure, designing
# on data you intend to score on can leak information.
preds <- rep(NA_real_, nrow(dTrain))
for(ci in crossValPlan) {
  cfe <- vtreat::mkCrossFrameCExperiment(dTrain[ci$train, , drop=FALSE], 
                                     vars,
                                     outcome, target)
  tplan <- cfe$treatments
  sf <- tplan$scoreFrame
  newvars <- sf$varName[sf$sig < 1/nrow(sf)]
  trainVtreat <- cfe$crossFrame[ , c(newvars, outcome), drop=FALSE]
  nrounds <- 1000
  cv <- xgb.cv(data= as.matrix(trainVtreat[, newvars, drop=FALSE]),
                   label= trainVtreat[[outcome]]==target,
                   objective= 'binary:logistic',
                   nrounds= nrounds,
                   verbose= 0,
                   nfold= 5)
  #nrounds  <- which.min(cv$evaluation_log$test_rmse_mean) # regression
  nrounds  <- which.min(cv$evaluation_log$test_error_mean) # classification
  model <- xgboost(data= as.matrix(trainVtreat[, newvars, drop=FALSE]),
                   label= trainVtreat[[outcome]]==target,
                   objective= 'binary:logistic',
                   nrounds= nrounds,
                   verbose= 0)
  appVtreat <- vtreat::prepare(tplan, 
                               dTrain[ci$app, , drop=FALSE], 
                               varRestriction = newvars)
  preds[ci$app] <-  predict(model,
                            as.matrix(appVtreat[, newvars, drop=FALSE]))
}
dTrain$predVtreatC <- preds
sigr::permTestAUC(dTrain, 
                  'predVtreatC',
                  outcome, target)
WVPlots::ROCPlot(dTrain, 
                 'predVtreatC', 
                 outcome, target, 
                 'vtreat y-aware encoder performance')
```
